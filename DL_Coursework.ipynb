{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tade5250/WeizActionClassifier/blob/main/DL_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ysqIt5DvJMDT",
      "metadata": {
        "id": "ysqIt5DvJMDT"
      },
      "source": [
        "# Deep Learning Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qh7C2lFBQnjz",
      "metadata": {
        "id": "Qh7C2lFBQnjz"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0KWxs8m4Q4uc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KWxs8m4Q4uc",
        "outputId": "5196a796-d590-4e36-e2f0-0f57de54c56b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.6.0 (from torchvision)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cusparselt-cu12-0.6.2 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n",
            "Collecting av\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n",
        "!pip install --upgrade torchvision\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "jDFfQzfUQ9ac",
      "metadata": {
        "id": "jDFfQzfUQ9ac"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_video, write_video\n",
        "import os\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import functional as TVF\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.io import read_video\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "XYcvJ5QoRgcL",
      "metadata": {
        "id": "XYcvJ5QoRgcL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Flag"
      ],
      "metadata": {
        "id": "CPi84ppQibmN"
      },
      "id": "CPi84ppQibmN"
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING = False"
      ],
      "metadata": {
        "id": "4HuiRXiriamP"
      },
      "id": "4HuiRXiriamP",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DKq_IrxM4JgU",
      "metadata": {
        "id": "DKq_IrxM4JgU"
      },
      "source": [
        "## Step 1: Getting the Data and Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "NRBB6A3KTUEm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NRBB6A3KTUEm",
        "outputId": "15aeef2d-9b57-45b5-b086-c45598b875a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "wiyIPD6Y4vdu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wiyIPD6Y4vdu",
        "outputId": "1635ff8f-d3c5-4cc9-c88f-b22d49314da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/dataset.zip\n",
            "   creating: /content/dataset/videos/\n",
            "   creating: /content/dataset/videos/run/\n",
            "  inflating: /content/dataset/videos/run/daria_run.avi  \n",
            "  inflating: /content/dataset/videos/run/ido_run.avi  \n",
            "  inflating: /content/dataset/videos/run/lena_run1.avi  \n",
            "  inflating: /content/dataset/videos/run/denis_run.avi  \n",
            "  inflating: /content/dataset/videos/run/eli_run.avi  \n",
            "  inflating: /content/dataset/videos/run/moshe_run.avi  \n",
            "  inflating: /content/dataset/videos/run/lena_run2.avi  \n",
            "  inflating: /content/dataset/videos/run/lyova_run.avi  \n",
            "  inflating: /content/dataset/videos/run/ira_run.avi  \n",
            "  inflating: /content/dataset/videos/run/shahar_run.avi  \n",
            "   creating: /content/dataset/videos/wave1/\n",
            "  inflating: /content/dataset/videos/wave1/ira_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/ido_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/lena_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/lyova_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/moshe_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/eli_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/denis_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/shahar_wave1.avi  \n",
            "  inflating: /content/dataset/videos/wave1/daria_wave1.avi  \n",
            "   creating: /content/dataset/videos/jump/\n",
            "  inflating: /content/dataset/videos/jump/ira_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/daria_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/moshe_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/denis_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/shahar_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/lyova_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/lena_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/ido_jump.avi  \n",
            "  inflating: /content/dataset/videos/jump/eli_jump.avi  \n",
            "   creating: /content/dataset/videos/jack/\n",
            "  inflating: /content/dataset/videos/jack/denis_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/lena_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/ido_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/moshe_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/ira_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/daria_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/eli_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/shahar_jack.avi  \n",
            "  inflating: /content/dataset/videos/jack/lyova_jack.avi  \n",
            "   creating: /content/dataset/videos/side/\n",
            "  inflating: /content/dataset/videos/side/lena_side.avi  \n",
            "  inflating: /content/dataset/videos/side/lyova_side.avi  \n",
            "  inflating: /content/dataset/videos/side/eli_side.avi  \n",
            "  inflating: /content/dataset/videos/side/ira_side.avi  \n",
            "  inflating: /content/dataset/videos/side/daria_side.avi  \n",
            "  inflating: /content/dataset/videos/side/ido_side.avi  \n",
            "  inflating: /content/dataset/videos/side/denis_side.avi  \n",
            "  inflating: /content/dataset/videos/side/shahar_side.avi  \n",
            "  inflating: /content/dataset/videos/side/moshe_side.avi  \n",
            "   creating: /content/dataset/videos/wave2/\n",
            "  inflating: /content/dataset/videos/wave2/ira_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/daria_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/lyova_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/denis_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/shahar_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/ido_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/moshe_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/lena_wave2.avi  \n",
            "  inflating: /content/dataset/videos/wave2/eli_wave2.avi  \n",
            "   creating: /content/dataset/videos/skip/\n",
            "  inflating: /content/dataset/videos/skip/lena_skip2.avi  \n",
            "  inflating: /content/dataset/videos/skip/lena_skip1.avi  \n",
            "  inflating: /content/dataset/videos/skip/shahar_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/moshe_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/denis_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/daria_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/lyova_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/eli_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/ido_skip.avi  \n",
            "  inflating: /content/dataset/videos/skip/ira_skip.avi  \n",
            "   creating: /content/dataset/videos/walk/\n",
            "  inflating: /content/dataset/videos/walk/moshe_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/denis_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/ido_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/ira_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/lyova_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/eli_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/lena_walk1.avi  \n",
            "  inflating: /content/dataset/videos/walk/shahar_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/daria_walk.avi  \n",
            "  inflating: /content/dataset/videos/walk/lena_walk2.avi  \n",
            "   creating: /content/dataset/videos/pjump/\n",
            "  inflating: /content/dataset/videos/pjump/ira_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/denis_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/lyova_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/shahar_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/ido_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/moshe_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/eli_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/daria_pjump.avi  \n",
            "  inflating: /content/dataset/videos/pjump/lena_pjump.avi  \n",
            "   creating: /content/dataset/videos/bend/\n",
            "  inflating: /content/dataset/videos/bend/ira_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/lyova_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/denis_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/moshe_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/daria_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/lena_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/shahar_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/ido_bend.avi  \n",
            "  inflating: /content/dataset/videos/bend/eli_bend.avi  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/dataset.zip -d /content/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u3TJpljy8dOV",
      "metadata": {
        "id": "u3TJpljy8dOV"
      },
      "source": [
        "## Step 2: Process the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "xEUZ-9FnUG7n",
      "metadata": {
        "id": "xEUZ-9FnUG7n"
      },
      "outputs": [],
      "source": [
        "classes = {\n",
        "    0: 'walk',\n",
        "    1: 'run',\n",
        "    2: 'jump',\n",
        "    3: 'side',\n",
        "    4: 'bend',\n",
        "    5: 'wave1',\n",
        "    6: 'wave2',\n",
        "    7: 'pjump',\n",
        "    8: 'jack',\n",
        "    9: 'skip'\n",
        "}\n",
        "\n",
        "# Reverse mapping for class_to_idx\n",
        "class_to_idx = {v: k for k, v in classes.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "xhV4WEssLbXB",
      "metadata": {
        "id": "xhV4WEssLbXB"
      },
      "outputs": [],
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, min_seq_length=10, max_seq_length=50):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.min_seq_length = min_seq_length\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.classes = classes\n",
        "        self.video_files = []\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "        subdirs = sorted(os.listdir(root_dir))\n",
        "        for cls_name in subdirs:\n",
        "            if cls_name not in self.class_to_idx:\n",
        "                raise ValueError(f\"Unexpected class directory: {cls_name}. Expected one of {list(self.class_to_idx.keys())}\")\n",
        "\n",
        "        for class_label in self.class_to_idx.keys():\n",
        "            class_dir = os.path.join(root_dir, class_label)\n",
        "            if not os.path.exists(class_dir):\n",
        "                raise ValueError(f\"Class directory not found: {class_dir}\")\n",
        "            for video_file in os.listdir(class_dir):\n",
        "                if video_file.endswith('.avi'):\n",
        "                    self.video_files.append((os.path.join(class_dir, video_file), class_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, class_label = self.video_files[idx]\n",
        "        video_avi, _, fps = read_video(video_path, pts_unit='sec')\n",
        "\n",
        "        total_frames = video_avi.shape[0]\n",
        "\n",
        "        # Sample a random sequence length\n",
        "        seq_length = random.randint(self.min_seq_length, self.max_seq_length)\n",
        "        if total_frames > seq_length:\n",
        "            start_idx = random.randint(0, total_frames - seq_length)\n",
        "            subsequence = video_avi[start_idx:start_idx + seq_length]\n",
        "        else:\n",
        "            start_idx = 0\n",
        "            seq_length = total_frames\n",
        "            subsequence = video_avi\n",
        "\n",
        "        subsequence = subsequence.permute(0, 3, 1, 2).float() / 255.0\n",
        "\n",
        "        # Apply transforms\n",
        "        augmented = False\n",
        "        if self.transform:\n",
        "            augmented = True\n",
        "            subsequence = self.transform(subsequence)\n",
        "\n",
        "        metadata = {\n",
        "            'class_label': class_label,\n",
        "            'class_index': self.class_to_idx[class_label],\n",
        "            'original_length': total_frames,\n",
        "            'subsequence_length': seq_length,\n",
        "            'start_index': start_idx,\n",
        "            'augmented': augmented\n",
        "        }\n",
        "\n",
        "        #print(\"Metadata:\", metadata)\n",
        "\n",
        "        return subsequence, torch.tensor(self.class_to_idx[class_label]), metadata\n",
        "\n",
        "class TransformSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y, metadata = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            metadata['applied_transforms'] = str(self.transform)\n",
        "        return x, y, metadata\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mJwoW2vV8iPr",
      "metadata": {
        "id": "mJwoW2vV8iPr"
      },
      "source": [
        "## Step 3: Split the Data into Train, Validate and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "SkRuQCPcyUMh",
      "metadata": {
        "id": "SkRuQCPcyUMh"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    videos, labels, metadata = zip(*batch)\n",
        "\n",
        "    lengths = [vid.shape[0] for vid in videos]\n",
        "    max_length = max(lengths)\n",
        "\n",
        "    padded_videos = []\n",
        "    for vid in videos:\n",
        "        l = vid.shape[0]\n",
        "        if l < max_length:\n",
        "            pad = torch.zeros((max_length - l, *vid.shape[1:]), dtype=vid.dtype)\n",
        "            vid = torch.cat([vid, pad], dim=0)\n",
        "        padded_videos.append(vid)\n",
        "\n",
        "    videos_tensor = torch.stack(padded_videos, dim=0)\n",
        "\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    lengths_tensor = torch.tensor(lengths)\n",
        "\n",
        "    return videos_tensor, labels_tensor, lengths_tensor, metadata\n",
        "\n",
        "def random_crop(video, crop_size):\n",
        "    h, w = video.shape[2], video.shape[3]\n",
        "    th, tw = crop_size\n",
        "    i = random.randint(0, h - th)\n",
        "    j = random.randint(0, w - tw)\n",
        "    return video[:, :, i:i+th, j:j+tw]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: random_crop(x, (128, 128))),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: random_crop(x, (128, 128))),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/dataset/videos'\n",
        "base_dataset = VideoDataset(root_dir=root_dir, transform=train_transform)\n",
        "\n",
        "labels = [base_dataset[i][1] for i in range(len(base_dataset))]\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "train_val_idx, test_idx = next(sss.split(range(len(base_dataset)), labels))\n",
        "\n",
        "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(sss_val.split(train_val_idx, [labels[i] for i in train_val_idx]))\n",
        "\n",
        "train_dataset = TransformSubset(Subset(base_dataset, train_idx), train_transform)\n",
        "val_dataset = TransformSubset(Subset(base_dataset, val_idx), val_test_transform)\n",
        "test_dataset = TransformSubset(Subset(base_dataset, test_idx), val_test_transform)\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36118b42-9e86-429c-d55c-7c2ce303484c",
        "id": "wjFN0QujKVZz"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 66\n",
            "Validation set size: 17\n",
            "Test set size: 10\n"
          ]
        }
      ],
      "id": "wjFN0QujKVZz"
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    videos, labels, lengths, metadata = batch\n",
        "    print(\"Train videos shape:\", videos.shape) # [batch_size, max_seq_length, C, H, W]\n",
        "    print(\"Train labels shape:\", labels.shape) # [batch_size]\n",
        "    print(\"Train lengths:\", lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTjebMBoJPJt",
        "outputId": "5478210b-4ba6-4c69-9384-0c2c10c24291"
      },
      "id": "KTjebMBoJPJt",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train videos shape: torch.Size([8, 48, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([29, 43, 25, 24, 28, 16, 48, 25])\n",
            "Train videos shape: torch.Size([8, 49, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([24, 30, 28, 49, 21, 33, 41, 39])\n",
            "Train videos shape: torch.Size([8, 42, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([31, 19, 24, 22, 10, 42, 41, 40])\n",
            "Train videos shape: torch.Size([8, 48, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([25, 17, 38, 31, 47, 35, 48, 41])\n",
            "Train videos shape: torch.Size([8, 48, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([15, 48, 30, 23, 18, 48, 23, 17])\n",
            "Train videos shape: torch.Size([8, 48, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([27, 38, 44, 42, 48, 48, 32, 14])\n",
            "Train videos shape: torch.Size([8, 36, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([16, 31, 36, 24, 28, 17, 18, 19])\n",
            "Train videos shape: torch.Size([8, 46, 3, 128, 128])\n",
            "Train labels shape: torch.Size([8])\n",
            "Train lengths: tensor([11, 46, 16, 21, 40, 28, 32, 25])\n",
            "Train videos shape: torch.Size([2, 33, 3, 128, 128])\n",
            "Train labels shape: torch.Size([2])\n",
            "Train lengths: tensor([13, 33])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.2: View the augmentations"
      ],
      "metadata": {
        "id": "ewq3zgIoQpSJ"
      },
      "id": "ewq3zgIoQpSJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_frames(video_tensor, metadata):\n",
        "    label_info = f\"Label: {metadata['class_index']} ({metadata['class_label']})\"\n",
        "    seq_info = (f\"Original length: {metadata['original_length']} frames, \"\n",
        "                f\"Subsequence: {metadata['subsequence_length']} frames (start idx: {metadata['start_index']}), \"\n",
        "                f\"augmented: {metadata['augmented']}\")\n",
        "    transforms_info = f\"Transforms: {metadata['applied_transforms']}\"\n",
        "    title = f\"{label_info}\\n{seq_info}\\n{transforms_info}\"\n",
        "\n",
        "    video_tensor = (video_tensor.permute(0, 2, 3, 1)) * 255.0\n",
        "    video_tensor = video_tensor.clamp(0, 255)\n",
        "\n",
        "    num_frames = video_tensor.shape[0]\n",
        "    cols = 5\n",
        "    rows = (num_frames // cols) + (1 if num_frames % cols != 0 else 0)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    axes = axes.ravel() if rows > 1 else [axes]\n",
        "    for i in range(num_frames):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(video_tensor[i].byte().numpy())\n",
        "        ax.axis('off')\n",
        "    # Hide any extra subplots\n",
        "    for i in range(num_frames, rows * cols):\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KoML_ICDQwHY"
      },
      "id": "KoML_ICDQwHY",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample, train_label, train_metadata = train_dataset[1]\n",
        "\n",
        "# # Visualize a few samples from the training set:\n",
        "# for i, (sample, label, metadata) in enumerate(train_dataset):\n",
        "#         if i < 3:\n",
        "#           continue\n",
        "#         if i < 6:\n",
        "#           show_frames(sample, metadata)\n",
        "#         else:\n",
        "#           break\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c3BQYr_CQy73"
      },
      "id": "c3BQYr_CQy73",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training sample shape:\", train_sample.shape)\n",
        "print(\"Training sample min value:\", train_sample.min().item())\n",
        "print(\"Training sample max value:\", train_sample.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QCx0F-fYd65",
        "outputId": "bd61c8e9-ee25-47ad-b4c7-6dc7f45f4518",
        "collapsed": true
      },
      "id": "0QCx0F-fYd65",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample shape: torch.Size([23, 3, 128, 128])\n",
            "Training sample min value: 0.0235294122248888\n",
            "Training sample max value: 0.6823529601097107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RLXOFUhe8rFi",
      "metadata": {
        "id": "RLXOFUhe8rFi"
      },
      "source": [
        "## Step 4: ConvLSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xmOsvYL9835D",
      "metadata": {
        "id": "xmOsvYL9835D"
      },
      "source": [
        "### Step 4.1: Construct the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.conv(x) + self.shortcut(x))\n",
        "\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.conv1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "      # Residual blocks\n",
        "      self.conv2 = self._make_residual_layer(64, 128, stride=2)\n",
        "      self.conv3 = self._make_residual_layer(128, 256, stride=2)\n",
        "      self.conv4 = self._make_residual_layer(256, 512, stride=2)\n",
        "      self.conv5 = self._make_residual_layer(512, 512, stride=2)\n",
        "\n",
        "      self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def _make_residual_layer(self, in_channels, out_channels, stride):\n",
        "      return nn.Sequential(\n",
        "          ResidualBlock(in_channels, out_channels, stride),\n",
        "          ResidualBlock(out_channels, out_channels, 1)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv5(x)\n",
        "      x = self.global_avg_pool(x)\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "3obEnok1aq9c"
      },
      "id": "3obEnok1aq9c",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "A5hq2lxoYprw",
      "metadata": {
        "id": "A5hq2lxoYprw"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, lstm_outputs):\n",
        "        attn_scores = self.attn(lstm_outputs)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        context = torch.sum(lstm_outputs * attn_weights, dim=1)\n",
        "        return context\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=256, num_layers=1, bidirectional=True, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.cnn = CNNFeatureExtractor()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True,\n",
        "                            bidirectional=bidirectional,\n",
        "                            dropout=dropout_p if num_layers > 1 else 0)\n",
        "\n",
        "        self.attention = Attention(hidden_size * self.num_directions)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Classification\n",
        "        self.fc = nn.Linear(hidden_size * self.num_directions, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.size()\n",
        "\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        features = self.cnn(x)\n",
        "        features = features.view(B, T, 512)\n",
        "\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(features)\n",
        "        context = self.attention(lstm_out)\n",
        "\n",
        "        context = self.dropout(context)\n",
        "        out = self.fc(context)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gaRjyNn_8-tR",
      "metadata": {
        "id": "gaRjyNn_8-tR"
      },
      "source": [
        "### Step 4.2: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNLSTM(num_classes=10, hidden_size=256, num_layers=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "early_stopping_patience = 7\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "num_epochs = 100\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for videos, labels, lengths, metadata in loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(videos)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * videos.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (videos, labels, lengths, metadata) in enumerate(loader):\n",
        "            videos = videos.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * videos.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Print predictions and labels for the current batch\n",
        "            # print(f\"Batch {batch_idx + 1}:\")\n",
        "            # print(\"Predictions:\", predicted.cpu().numpy())\n",
        "            # print(\"Labels:     \", labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Main training loop\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "best_model_wts = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(best_model_wts, 'CNN_LSTM.pth')\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "if best_model_wts is not None:\n",
        "    model.load_state_dict(torch.load('CNN_LSTM.pth'))\n",
        "\n",
        "test_loss, test_acc = validate_epoch(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hUUaIB6eUBca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0f87db-d716-4b0c-c6f3-825aca59874b"
      },
      "id": "hUUaIB6eUBca",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/100]\n",
            "Train Loss: 2.3008, Train Acc: 0.0606 | Val Loss: 2.3344, Val Acc: 0.0000\n",
            "\n",
            "Epoch [2/100]\n",
            "Train Loss: 2.2266, Train Acc: 0.0909 | Val Loss: 2.3975, Val Acc: 0.0588\n",
            "\n",
            "Epoch [3/100]\n",
            "Train Loss: 2.1551, Train Acc: 0.2273 | Val Loss: 2.4575, Val Acc: 0.0588\n",
            "\n",
            "Epoch [4/100]\n",
            "Train Loss: 1.9751, Train Acc: 0.3636 | Val Loss: 2.4710, Val Acc: 0.0000\n",
            "\n",
            "Epoch [5/100]\n",
            "Train Loss: 1.7889, Train Acc: 0.4394 | Val Loss: 2.4698, Val Acc: 0.0588\n",
            "\n",
            "Epoch [6/100]\n",
            "Train Loss: 1.6530, Train Acc: 0.4091 | Val Loss: 2.2719, Val Acc: 0.1176\n",
            "\n",
            "Epoch [7/100]\n",
            "Train Loss: 1.5869, Train Acc: 0.3485 | Val Loss: 1.9532, Val Acc: 0.1176\n",
            "\n",
            "Epoch [8/100]\n",
            "Train Loss: 1.4525, Train Acc: 0.4545 | Val Loss: 1.9920, Val Acc: 0.1765\n",
            "\n",
            "Epoch [9/100]\n",
            "Train Loss: 1.3946, Train Acc: 0.4697 | Val Loss: 2.0449, Val Acc: 0.2353\n",
            "\n",
            "Epoch [10/100]\n",
            "Train Loss: 1.2729, Train Acc: 0.4697 | Val Loss: 1.8299, Val Acc: 0.2941\n",
            "\n",
            "Epoch [11/100]\n",
            "Train Loss: 1.2547, Train Acc: 0.4848 | Val Loss: 1.7798, Val Acc: 0.1765\n",
            "\n",
            "Epoch [12/100]\n",
            "Train Loss: 1.2727, Train Acc: 0.5909 | Val Loss: 1.8443, Val Acc: 0.1765\n",
            "\n",
            "Epoch [13/100]\n",
            "Train Loss: 1.2878, Train Acc: 0.4848 | Val Loss: 1.4230, Val Acc: 0.5882\n",
            "\n",
            "Epoch [14/100]\n",
            "Train Loss: 1.2536, Train Acc: 0.4848 | Val Loss: 1.5585, Val Acc: 0.2941\n",
            "\n",
            "Epoch [15/100]\n",
            "Train Loss: 1.2321, Train Acc: 0.5606 | Val Loss: 1.5394, Val Acc: 0.2941\n",
            "\n",
            "Epoch [16/100]\n",
            "Train Loss: 1.2582, Train Acc: 0.5758 | Val Loss: 1.2906, Val Acc: 0.5294\n",
            "\n",
            "Epoch [17/100]\n",
            "Train Loss: 1.2460, Train Acc: 0.5303 | Val Loss: 1.4580, Val Acc: 0.4118\n",
            "\n",
            "Epoch [18/100]\n",
            "Train Loss: 1.1380, Train Acc: 0.5455 | Val Loss: 1.4406, Val Acc: 0.4118\n",
            "\n",
            "Epoch [19/100]\n",
            "Train Loss: 1.1327, Train Acc: 0.6212 | Val Loss: 1.6127, Val Acc: 0.3529\n",
            "\n",
            "Epoch [20/100]\n",
            "Train Loss: 0.9652, Train Acc: 0.7121 | Val Loss: 1.6757, Val Acc: 0.4118\n",
            "\n",
            "Epoch [21/100]\n",
            "Train Loss: 1.0247, Train Acc: 0.6515 | Val Loss: 1.3552, Val Acc: 0.5294\n",
            "\n",
            "Epoch [22/100]\n",
            "Train Loss: 1.0335, Train Acc: 0.6667 | Val Loss: 1.3081, Val Acc: 0.5294\n",
            "\n",
            "Epoch [23/100]\n",
            "Train Loss: 0.9458, Train Acc: 0.6212 | Val Loss: 1.1703, Val Acc: 0.5882\n",
            "\n",
            "Epoch [24/100]\n",
            "Train Loss: 0.8662, Train Acc: 0.7273 | Val Loss: 1.2404, Val Acc: 0.5882\n",
            "\n",
            "Epoch [25/100]\n",
            "Train Loss: 1.1202, Train Acc: 0.6667 | Val Loss: 1.2703, Val Acc: 0.4118\n",
            "\n",
            "Epoch [26/100]\n",
            "Train Loss: 0.8807, Train Acc: 0.6970 | Val Loss: 1.0923, Val Acc: 0.5882\n",
            "\n",
            "Epoch [27/100]\n",
            "Train Loss: 0.9206, Train Acc: 0.6818 | Val Loss: 1.2366, Val Acc: 0.4706\n",
            "\n",
            "Epoch [28/100]\n",
            "Train Loss: 0.8540, Train Acc: 0.6818 | Val Loss: 1.4182, Val Acc: 0.4118\n",
            "\n",
            "Epoch [29/100]\n",
            "Train Loss: 0.9161, Train Acc: 0.7424 | Val Loss: 1.1132, Val Acc: 0.4706\n",
            "\n",
            "Epoch [30/100]\n",
            "Train Loss: 0.6838, Train Acc: 0.7576 | Val Loss: 1.2973, Val Acc: 0.4706\n",
            "\n",
            "Epoch [31/100]\n",
            "Train Loss: 0.6884, Train Acc: 0.7576 | Val Loss: 1.4305, Val Acc: 0.4706\n",
            "\n",
            "Epoch [32/100]\n",
            "Train Loss: 0.6435, Train Acc: 0.7727 | Val Loss: 1.1300, Val Acc: 0.5882\n",
            "\n",
            "Epoch [33/100]\n",
            "Train Loss: 0.7877, Train Acc: 0.7424 | Val Loss: 1.2130, Val Acc: 0.5882\n",
            "Early stopping triggered!\n",
            "Training complete.\n",
            "Test Loss: 1.0678, Test Accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KqrEN1Omhx_n",
      "metadata": {
        "id": "KqrEN1Omhx_n"
      },
      "source": [
        "### Step 4.3: Test Best Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3_DYZppM1G4"
      },
      "id": "R3_DYZppM1G4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "zTi_mKsB9vgw",
      "metadata": {
        "id": "zTi_mKsB9vgw"
      },
      "source": [
        "### Step 4.3: Save the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_oPD4we59QgQ",
      "metadata": {
        "id": "_oPD4we59QgQ"
      },
      "source": [
        "## Step 5: Loading the Pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5465--Zb96be",
      "metadata": {
        "id": "5465--Zb96be"
      },
      "source": [
        "### Step 5.1: Apply model to Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gMP9vY5L-CA5",
      "metadata": {
        "id": "gMP9vY5L-CA5"
      },
      "source": [
        "### Step 5.2: Analyse and Display the Results and Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MVVL-65o-N5U",
      "metadata": {
        "id": "MVVL-65o-N5U"
      },
      "source": [
        "# Report\n",
        "\n",
        "**Student Number: 2619191a**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8OKD3KvAJjNT",
      "metadata": {
        "id": "8OKD3KvAJjNT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ewq3zgIoQpSJ",
        "_oPD4we59QgQ"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}